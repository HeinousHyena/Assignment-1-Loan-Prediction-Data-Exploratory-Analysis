

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from google.colab import files
files.upload()

# 1. Use regression techniques

df = pd.read_csv('tips.csv')
print(df.info())
print(df.head())
print(df.isnull().sum())
print(df.describe())

numeric_df = df.select_dtypes(include=[np.number])
plt.figure(figsize=(10, 6))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap")
plt.show()

sns.pairplot(df, hue='sex')
plt.show()
sns.boxplot(x='day', y='tip', data=df)
plt.show()
sns.boxplot(x='time', y='tip', data=df)
plt.show()
sns.countplot(x='day', hue='smoker', data=df)
plt.show()
X = df.drop('tip', axis=1)
y = df['tip']
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['total_bill', 'size']),
        ('cat', OneHotEncoder(drop='first'), ['sex', 'smoker', 'day', 'time'])
    ])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Split data into features and target
X = df.drop('tip', axis=1)
y = df['tip']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['total_bill', 'size']),
        ('cat', OneHotEncoder(drop='first'), ['sex', 'smoker', 'day', 'time'])
    ])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Function to evaluate model performance
def evaluate_model(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"RMSE: {rmse}, MAE: {mae}, R²: {r2}")
    return rmse, mae, r2

models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=1.0),
    'Lasso Regression': Lasso(alpha=0.1),
    'Decision Tree': DecisionTreeRegressor(random_state=42),
    'Random Forest': RandomForestRegressor(random_state=42, n_estimators=100),
    'Support Vector Regressor': SVR(kernel='rbf'),
    'K-Nearest Neighbors': KNeighborsRegressor(n_neighbors=5)
}

# Pipeline for transforming and modeling
for model_name, model in models.items():
    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])
    print(f"\n{model_name} Results:")
    evaluate_model(pipeline, X_train, X_test, y_train, y_test)

for model_name, model in models.items():
    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])
    scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
    rmse_scores = np.sqrt(-scores)
    print(f"{model_name} Cross-Validation RMSE: {rmse_scores.mean():.2f} ± {rmse_scores.std():.2f}")

pipeline_rf = Pipeline(steps=[('preprocessor', preprocessor), ('model', RandomForestRegressor(random_state=42, n_estimators=100))])
pipeline_rf.fit(X_train, y_train)

model_rf = pipeline_rf.named_steps['model']
feature_importances = model_rf.feature_importances_
feature_names = preprocessor.transformers_[0][2] + list(preprocessor.named_transformers_['cat'].get_feature_names_out())
feature_imp_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})

feature_imp_df.sort_values(by='Importance', ascending=False, inplace=True)
sns.barplot(x='Importance', y='Feature', data=feature_imp_df)
plt.title('Feature Importance from Random Forest')
plt.show()
"""
Using the feature importances and analysis from previous steps, summarize key insights:

High Impact Features: Highlight factors like total_bill or size.
Tipping Patterns: Summarize patterns across days, times, or smoker status.
Service Recommendations: Tailor suggestions for improved customer service strategies. 
"""

# 2. Plots and apllication of suitable regression technique

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 5))
sns.scatterplot(x='total_bill', y='tip', data=df)
plt.title("Scatter Plot of Total Bill vs Tip")
plt.xlabel("Total Bill")
plt.ylabel("Tip")
plt.show()

plt.figure(figsize=(8, 5))
sns.scatterplot(x='size', y='tip', data=df)
plt.title("Scatter Plot of Size vs Tip")
plt.xlabel("Size")
plt.ylabel("Tip")
plt.show()

sns.pairplot(df, diag_kind='kde', plot_kws={'alpha':0.5})
plt.suptitle("Pair Plot of Multiple Features", y=1.02)
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

numeric_df = df.select_dtypes(include=[np.number])  

plt.figure(figsize=(10, 6))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title("Correlation Matrix (Heatmap)")
plt.show()

import statsmodels.api as sm
from statsmodels.stats.diagnostic import linear_rainbow

# Prepare data for regression
X = df[['total_bill', 'size']]  # Continuous variables for simplicity
X = sm.add_constant(X)  # Add constant term for intercept
y = df['tip']

# Fit OLS regression model
model = sm.OLS(y, X).fit()

# Perform Rainbow test
rainbow_statistic, rainbow_p_value = linear_rainbow(model)
print(f"Rainbow Test p-value: {rainbow_p_value}")

# Residuals plot
predictions = model.predict(X)
residuals = y - predictions

plt.figure(figsize=(8, 5))
sns.residplot(x=predictions, y=residuals, lowess=True, line_kws={'color': 'red'})
plt.title("Residuals Plot")
plt.xlabel("Predicted Tip")
plt.ylabel("Residuals")
plt.axhline(0, color='gray', linestyle='--')
plt.show()

plt.figure(figsize=(10, 6))
sns.lineplot(x='time', y='tip', data=df)  # Replace 'time_column_name'
plt.title("Line Plot of Tip over Time")
plt.xlabel("Time")
plt.ylabel("Tip")
plt.show()

# So Random Forest Regression will be the best choice asHandles Categorical and Numerical Data Well: It works effectively with both continuous and categorical variables like Total_bill, size, day, and time.
# Non-linear Relationships: Random Forest captures non-linear relationships and interactions between variables without requiring explicit feature engineering.

# Random Forest Implementation
from sklearn.preprocessing import StandardScaler
ss=StandardScaler()
x=data.drop(["tip"],axis=1)
y=data["tip"]
x=ss.fit_transform(x)
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
regressor=RandomForestRegressor()
paramaters={'criterion':['squared_error','absolute_error','friedman_mse','poisson']}
forest_regressor=GridSearchCV(regressor,paramaters,scoring='accuracy',cv=5)
forest_regressor.fit(x_train,y_train)
forest_regressor.best_params_
predictions=forest_regressor.predict(x_train)
predictions.flatten()
from sklearn.metrics import r2_score
score=r2_score(y_train,predictions)
print(score*100)
